{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example_v0 (gym.Env):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_MIN = 1\n",
    "RT_MAX = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVE_LF = 0\n",
    "MOVE_RT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 10\n",
    "                                \n",
    "REWARD_AWAY = -2\n",
    "REWARD_STEP = -1\n",
    "REWARD_GOAL = MAX_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"render.modes\": [\"human\"]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__ (self):\n",
    "    self.action_space = gym.spaces.Discrete(2)\n",
    "    self.observation_space = gym.spaces.Discrete(self.RT_MAX + 1)\n",
    "    \n",
    "    # possible positions to chose on `reset()`                                                          \n",
    "    self.goal = int((self.LF_MIN + self.RT_MAX - 1) / 2)\n",
    "    self.init_positions = list(range(self.LF_MIN, self.RT_MAX))\n",
    "    self.init_positions.remove(self.goal)\n",
    "    \n",
    "    # change to guarantee the sequence of pseudorandom numbers\n",
    "    # (e.g., for debugging)                                                               \n",
    "    self.seed()\n",
    "    \n",
    "    self.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset (self):\n",
    "    self.position = self.np_random.choice(self.init_positions)\n",
    "    self.count = 0\n",
    "    \n",
    "    self.state = self.position\n",
    "    self.reward = 0\n",
    "    self.done = False\n",
    "    self.info = {}\n",
    "    \n",
    "    return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step (self, action):\n",
    "    if self.done:\n",
    "        # should never reach this point\n",
    "        print(\"EPISODE DONE!!!\")\n",
    "    elif self.count == self.MAX_STEPS:\n",
    "        self.done = True;\n",
    "    else:\n",
    "        assert self.action_space.contains(action)\n",
    "        self.count += 1\n",
    "\n",
    "        // insert simulation logic to handle an action ...\n",
    "\n",
    "    try:\n",
    "        assert self.observation_space.contains(self.state)\n",
    "    except AssertionError:\n",
    "        print(\"INVALID STATE\", self.state)\n",
    "\n",
    "    return [self.state, self.reward, self.done, self.info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if action == self.MOVE_LF:\n",
    "    if self.position == self.LF_MIN:\n",
    "        # invalid\n",
    "        self.reward = self.REWARD_AWAY\n",
    "    else:\n",
    "        self.position -= 1\n",
    "\n",
    "    if self.position == self.goal:\n",
    "        # on goal now\n",
    "        self.reward = self.REWARD_GOAL\n",
    "        self.done = 1\n",
    "    elif self.position < self.goal:\n",
    "        # moving away from goal\n",
    "        self.reward = self.REWARD_AWAY\n",
    "    else:\n",
    "        # moving toward goal\n",
    "        self.reward = self.REWARD_STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif action == self.MOVE_RT:\n",
    "    if self.position == self.RT_MAX:\n",
    "        # invalid\n",
    "        self.reward = self.REWARD_AWAY\n",
    "    else:\n",
    "        self.position += 1\n",
    "\n",
    "    if self.position == self.goal:\n",
    "        # on goal now\n",
    "        self.reward = self.REWARD_GOAL\n",
    "        self.done = 1\n",
    "    elif self.position > self.goal:\n",
    "        # moving away from goal\n",
    "        self.reward = self.REWARD_AWAY\n",
    "    else:\n",
    "        # moving toward goal\n",
    "        self.reward = self.REWARD_STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.state = self.position\n",
    "self.info[\"dist\"] = self.goal - self.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render (self, mode=\"human\"):\n",
    "    s = \"position: {:2d}  reward: {:2d}  info: {}\"\n",
    "    print(s.format(self.state, self.reward, self.info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed (self, seed=None):\n",
    "    self.np_random, seed = seeding.np_random(seed)\n",
    "    return [seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close (self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup\n",
    "\n",
    "setup(name=\"gym_example\",\n",
    "      version=\"1.0.0\",\n",
    "      install_requires=[\"gym\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id=\"example-v0\",\n",
    "    entry_point=\"gym_example.envs:Example_v0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_example.envs.example_env import Example_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_episode (env):\n",
    "    env.reset()\n",
    "    sum_reward = 0\n",
    "\n",
    "    for i in range(env.MAX_STEPS):\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return sum_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"example-v0\")\n",
    "sum_reward = run_one_episode(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    sum_reward = run_one_episode(env)\n",
    "    history.append(sum_reward)\n",
    "\n",
    "avg_sum_reward = sum(history) / len(history)\n",
    "print(\"\\nbaseline cumulative reward: {:6.2}\".format(avg_sum_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "chkpt_root = \"tmp/exa\"\n",
    "shutil.rmtree(chkpt_root, ignore_errors=True, onerror=None)\n",
    "\n",
    "ray_results = \"{}/ray_results/\".format(os.getenv(\"HOME\"))\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(ignore_reinit_error=True, local_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from gym_example.envs.example_env import Example_v0\n",
    "\n",
    "select_env = \"example-v0\"\n",
    "register_env(select_env, lambda config: Example_v0())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "agent = ppo.PPOTrainer(config, env=select_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = \"{:2d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:4.2f} saved {}\"\n",
    "n_iter = 5\n",
    "\n",
    "for n in range(n_iter):\n",
    "    result = agent.train()\n",
    "    chkpt_file = agent.save(chkpt_root)\n",
    "    print(status.format(\n",
    "            n + 1,\n",
    "            result[\"episode_reward_min\"],\n",
    "            result[\"episode_reward_mean\"],\n",
    "            result[\"episode_reward_max\"],\n",
    "            result[\"episode_len_mean\"],\n",
    "            chkpt_file\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=$HOME/ray_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "agent.restore(chkpt_file)\n",
    "env = gym.make(select_env)\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_reward = 0\n",
    "n_step = 20\n",
    "\n",
    "for step in range(n_step):\n",
    "    action = agent.compute_action(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "    sum_reward += reward\n",
    "\n",
    "    if done == 1:\n",
    "        print(\"cumulative reward\", sum_reward)\n",
    "        state = env.reset()\n",
    "        sum_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt\n",
    "pip install -e gym-example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
